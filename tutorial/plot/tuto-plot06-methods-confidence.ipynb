{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building confidence on explainability methods\n",
    "\n",
    "Local explainability methods are recent, they keep being improved and research is very active in the field. Thus, one needs to be careful when interpreting those methods' outputs. \n",
    "\n",
    "Depending on the tasks and data, explainability methods may give different results. The purpose of the metrics presented below is to assess the degree of confidence in these cases. We will answer the following questions: \n",
    "\n",
    "- Do different explainability methods give similar explanations on average ? (**Consistency**)\n",
    "\n",
    "- Is the explanation similar for similar instances ? (**Local stability**)\n",
    "\n",
    "- Do a few features drive the model ? (**Approximation**)\n",
    "\n",
    "<b>This short tutorial </b>presents an example of how those 3 metrics could be used on a classification case.\n",
    "\n",
    "We used Kaggle's [Titanic](https://www.kaggle.com/c/titanic/data) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from category_encoders import OrdinalEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/francescomallardmarini/Documents/Data_Analyst_FR/SoGeÃÅ/Teletravail/MAIF/shapash/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Supervized Model\n",
    "\n",
    "Let's start by loading a dataset and building a model that we will try to explain right after.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Titanic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapash.data.data_loader import data_loading\n",
    "titanic_df, titanic_dict = data_loading('titanic')\n",
    "del titanic_df['Name']\n",
    "y_df=titanic_df['Survived'].to_frame()\n",
    "X_df=titanic_df[titanic_df.columns.difference(['Survived'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Titanic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import OrdinalEncoder\n",
    "\n",
    "categorical_features = [col for col in X_df.columns if X_df[col].dtype == 'object']\n",
    "\n",
    "encoder = OrdinalEncoder(\n",
    "    cols=categorical_features,\n",
    "    handle_unknown='ignore',\n",
    "    return_df=True).fit(X_df)\n",
    "\n",
    "X_df=encoder.transform(X_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / Test Split + model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_df, y_df, train_size=0.75, random_state=7)\n",
    "\n",
    "# Subsample\n",
    "Xtrain = Xtrain[:50].reset_index(drop=True)\n",
    "ytrain = ytrain[:50].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier(n_estimators=200).fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select an explainability method\n",
    "\n",
    "Now that the model is ready, we need to pick an explainability method. As mentioned earlier, many of them exist, with different theoretical foundation, underlying assumptions, and levels of maturity. Thus, results might differ significantly among methods. Can we trust them?\n",
    "\n",
    "The **Consistency metric** compares methods between them and evaluates how close the explanations are from each other: if underlying assumptions lead to similar results, we would be more confident in using those methods. If not, careful conideration should be taken in the interpretation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we need to instantiate and compile the Consistency object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explainer.consistency import Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cns = Consistency()\n",
    "cns.compile(x=Xtrain, # Dataset for which we need explanations\n",
    "            model=clf, # Model to explain\n",
    "            preprocessing=encoder, # Optional\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now display the consistency plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cns.consistency_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each pair of explainability methods, the metric compares the explanations across the dataset (or a sample if specified) and calculates an average distance between the explainability methods. Two plots are proposed:\n",
    "\n",
    "* the first graph shows the aforementioned distances between methods on a 2D plan.\n",
    "    * As we can see here, \"shap\" and \"acv\" have very similar explanations, while \"lime\" provides consistently different explanations.\n",
    "\n",
    "* the second graph serves as a support for the first one: it gives a better sense of what the distances mean (what is 1.42? is it acceptable or not?). To do so, the metric extracts 5 real comparisons from the dataset (examples are represented by their Id in the dataframe) with distances similar to those in the first plot.\n",
    "    * As we can see, a distance of .04 gives very similar contributions between features, while 1.44 pretty much describes opposite behaviors.\n",
    "\n",
    "Depending on the selected methods, examples above show how different the contributions could be. Interpreting them must be done carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two parameters can be chosen in those graphs:\n",
    "* _selection:_ a sample of the dataset on which to evaluate the metric expressed as a list of indices (by default take the whole dataset if not too big)\n",
    "\n",
    "* *max_features*: the number of features displayed in the graph (the most significant ones are selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [45, 36, 12, 17, 29]\n",
    "cns.consistency_plot(selection=index, max_features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note:_ Instead of providing a dataset with a model, we can also calculate contributions beforehand and use those ones in the metric using the _contributions_ argument. This allows, for example, to use explainability methods that are not supported in Shapash. Let's for example calculate contributions separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "treeSHAP = shap.TreeExplainer(clf).shap_values(Xtrain[:50], check_additivity=False)[1]\n",
    "samplingSHAP = shap.SamplingExplainer(clf.predict_proba, shap.kmeans(Xtrain, 10)).shap_values(Xtrain[:50], check_additivity=False)[1]\n",
    "kernelSHAP = shap.KernelExplainer(clf.predict_proba, shap.kmeans(Xtrain, 10)).shap_values(Xtrain[:50], check_additivity=False)[1]\n",
    "\n",
    "treeSHAP = pd.DataFrame(treeSHAP, columns=Xtrain[:50].columns)\n",
    "samplingSHAP = pd.DataFrame(samplingSHAP, columns=Xtrain[:50].columns)\n",
    "kernelSHAP = pd.DataFrame(kernelSHAP, columns=Xtrain[:50].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The format must be a dictionary where keys are methods names and values are pandas DataFrames: be careful to have dataframes with same shape, index and column names\n",
    "contributions = {\"tree\\nSHAP\": treeSHAP, \"sampling\\nSHAP\": samplingSHAP, \"kernel\\nSHAP\":kernelSHAP}\n",
    "\n",
    "cns.compile(contributions=contributions)\n",
    "cns.consistency_plot(selection=index, max_features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the compacity of the explanations\n",
    "\n",
    "What if the model uses tens or hundreds of features? Is it still going to be easy to understand how it works? Probably not. Indeed, the number of features greatly affects explainability\n",
    "\n",
    "The **Compacity metric** measures how well each decision can be explained by relying on relatively few features, which may however be different from one instance to another\n",
    "\n",
    "The idea is the following: _for each instace, we select the features with the highest contributions and we look at how well they approximate the model. Results are then aggregated across the whole dataset (or a sample of it) and displayed_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we need to instantiate and compile a SmartExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explainer.smart_explainer import SmartExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict = {0: 'Death', 1:' Survival'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpl = SmartExplainer(features_dict=titanic_dict, # Optional parameters\n",
    "                     label_dict=response_dict) # Optional parameters, dicts specify labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xpl.compile(\n",
    "    x=Xtrain, # Dataset for which we need explanations\n",
    "    model=clf, # Model to explain\n",
    "    preprocessing=encoder, # Optional\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now display the consistency plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpl.plot.compacity_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to measure how well the sum of the most important contributions approximates the global decision of the model. The two graphs show the link between the level of approximation, the number of required features to reach it and the proportion of the dataset on which it works.\n",
    "\n",
    "* In the left graph, for example, we can read that top 4 features reach the default approximation for 93.3% of the instances. Thus, if we needed to provide accurate explanations about the model, a small subset of features will provide a reliable explanation for a vast majority of instances\n",
    "\n",
    "* In the right graph, for example, we can read that top 5 features reach 99% of the reference model for 28.6% of the instances. Thus, if we needed something extremely precise, we would probably need to consider all features in the explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple parameters can be modified, such as:\n",
    "\n",
    "* *selection*: a sample of the dataset on which to evaluate the metric expressed as a list of indices (by default take the whole dataset if not too big)\n",
    "\n",
    "* *approx*: how close we want to be the reference model with all features (default 90%) -- Left graph\n",
    "\n",
    "* *nb_features*: how many features are selected to evaluate the approximation (default 5) -- Right graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
    "\n",
    "xpl.plot.compacity_plot(selection=index, approx=.85, nb_features=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the stability of the explanations\n",
    "\n",
    "To increase confidence in the explanation, measuring their stability is important.\n",
    "\n",
    "We define stability as follows: _if instances are very similar, then one would expect the explanations to be similar as well.\n",
    "Therefore, locally stable explanations are an important factor that help build trust around a particular explanation._\n",
    "\n",
    "The similarity between instances is evaluated under two criteria: (1) the instances must be close in the feature space and (2) have similar model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can now display the stability plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpl.plot.stability_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot looks at the neighborhood around each provided instance (reminder: neighborhood in terms of features and model output) and shows:\n",
    "\n",
    "* the average importance of the feature across the dataset based on its contributions (y-axis)\n",
    "\n",
    "* the average variability of the feature across the instances' neighborhood (x-axis)\n",
    "\n",
    "Left features are stable in the neighborhood, unlike those on the right. Top features are important, unlike bottom ones\n",
    "\n",
    "* Here, features like \"Sex\", \"Title\" and \"Class\" in the left-hand side seem to have strong and relatively stable contributions, so one might be more confident in using them for explanations.\n",
    "* On the other hand, features like \"Fare\" and \"Port of embarcation\" are much more unstable, and we might want to be careful before interpreting explanations around those features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple parameters can be modified, such as:\n",
    "\n",
    "* *selection*: a sample of the dataset on which to evaluate the metric expressed as a list of indices (by default take the whole dataset if not too big)\n",
    "\n",
    "* *max_features*: the number of features displayed in the graph (the most significant ones are selected)\n",
    "\n",
    "* *distribution*: changes the type of displayed graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Means are not always representative of the individual behaviors. Thus, we might look at the distribution of variability as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [45, 36, 12, 17, 29]\n",
    "\n",
    "# Using distribution = \"boxplot\" or \"violin\" displays distributions of variability instead of means\n",
    "xpl.plot.stability_plot(selection=index, max_features=5, distribution=\"boxplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stability can also be evaluated for a single instance. In that case, contributions are directly compared against neighbors and displayed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpl.plot.local_neighbors_plot(index=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here how difficult it is to interpret contributions sometimes: the feature _Port of embarkation_ gives completely opposite suggestions in the same neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('xai': conda)",
   "language": "python",
   "name": "python37764bitxaiconda3ce2877c032a42108d206d321552a8dd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}